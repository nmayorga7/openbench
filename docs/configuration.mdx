---
title: "Configuration"
description: "Set OpenBench model and evaluation behavior"
---
## Configuration Methods

OpenBench can be configured through multiple methods, in order of precedence:

1. **Command-Line Arguments**

```bash
# Run eval with --config CLI flags
bench mmlu --max_tokens=10000 --temperature=0.5
```

See what else you can do with the [OpenBench CLI.](/cli/overview)  
  
2. **Environment Variables**

```bash
# Set configuration defaults
export BENCH_MAX_TOKENS=10000
export BENCH_TEMPERATURE=0.5
```

3. **Configuration Files** (`.env` files)  

```bash .env
# Write default values to config file
BENCH_MAX_TOKENS=10000
BENCH_TEMPERATURE=0.5
```

```bash
# Automatically loaded if in current directory
bench eval mmlu

# Or source manually
source .env
bench eval mmlu
```

If not defined through any of the above methods, parameters resort to **default values**.

## Configuration Parameters

### Model Providers

OpenBench supports 30+ model providers through Inspect AI. Set the appropriate API key environment variable:

| Provider              | Environment Variable   | Example Model String             |
| --------------------- | ---------------------- | -------------------------------- |
| **AI21 Labs**         | `AI21_API_KEY`         | `ai21/model-name`                |
| **Anthropic**         | `ANTHROPIC_API_KEY`    | `anthropic/model-name`           |
| **AWS Bedrock**       | AWS credentials        | `bedrock/model-name`             |
| **Azure**             | `AZURE_OPENAI_API_KEY` | `azure/<deployment-name>`        |
| **Baseten**           | `BASETEN_API_KEY`      | `baseten/model-name`             |
| **Cerebras**          | `CEREBRAS_API_KEY`     | `cerebras/model-name`            |
| **Cohere**            | `COHERE_API_KEY`       | `cohere/model-name`              |
| **Crusoe**            | `CRUSOE_API_KEY`       | `crusoe/model-name`              |
| **DeepInfra**         | `DEEPINFRA_API_KEY`    | `deepinfra/model-name`           |
| **Friendli**          | `FRIENDLI_TOKEN`       | `friendli/model-name`            |
| **Google**            | `GOOGLE_API_KEY`       | `google/model-name`              |
| **Groq**              | `GROQ_API_KEY`         | `groq/model-name`                |
| **Hugging Face**      | `HF_TOKEN`             | `huggingface/model-name`         |
| **Hyperbolic**        | `HYPERBOLIC_API_KEY`   | `hyperbolic/model-name`          |
| **Lambda**            | `LAMBDA_API_KEY`       | `lambda/model-name`              |
| **MiniMax**           | `MINIMAX_API_KEY`      | `minimax/model-name`             |
| **Mistral**           | `MISTRAL_API_KEY`      | `mistral/model-name`             |
| **Moonshot**          | `MOONSHOT_API_KEY`     | `moonshot/model-name`            |
| **Nebius**            | `NEBIUS_API_KEY`       | `nebius/model-name`              |
| **Nous Research**     | `NOUS_API_KEY`         | `nous/model-name`                |
| **Novita AI**         | `NOVITA_API_KEY`       | `novita/model-name`              |
| **Ollama**            | None (local)           | `ollama/model-name`              |
| **OpenAI**            | `OPENAI_API_KEY`       | `openai/model-name`              |
| **OpenRouter**        | `OPENROUTER_API_KEY`   | `openrouter/model-name`          |
| **Parasail**          | `PARASAIL_API_KEY`     | `parasail/model-name`            |
| **Perplexity**        | `PERPLEXITY_API_KEY`   | `perplexity/model-name`          |
| **Reka**              | `REKA_API_KEY`         | `reka/model-name`                |
| **SambaNova**         | `SAMBANOVA_API_KEY`    | `sambanova/model-name`           |
| **Together AI**       | `TOGETHER_API_KEY`     | `together/model-name`            |
| **Vercel AI Gateway** | `AI_GATEWAY_API_KEY`   | `vercel/creator-name/model-name` |
| **vLLM**              | None (local)           | `vllm/model-name`                |

### Model Configuration

| CLI Command        | Environment Variable   | Description                                   |
|--------------------|------------------------|-----------------------------------------------|
| `--model`          | `BENCH_MODEL`          | Model(s) to evaluate.                         |
| `--model-base-url` | `BENCH_MODEL_BASE_URL` | Base URL for model(s).                        |
| `--model-role`     | `BENCH_MODEL_ROLE`     | Map role(s) to specific models.               |

Use `-M` Flag for Other Model-Specific Arguments  
(e.g. `bench eval simpleqa --model openai/o3-2025-04-16 -M reasoning_effort=high`)

### Task Configuration

| CLI Command        | Environment Variable   | Description                                   |
|--------------------|------------------------|-----------------------------------------------|
| `--epochs`         | `BENCH_EPOCHS`         | Number of epochs to run each evaluation.      |
| `--limit`          | `BENCH_LIMIT`          | Limit evaluated samples (single number or range). |
| `--message-limit`  | `BENCH_MESSAGE_LIMIT`  | Maximum number of messages per sample.        |
| `--score / --no-score` | `BENCH_SCORE`      | Grade the benchmark, or skip scoring (can score later). |

Use `-T` Flag for Other Task-Specific Arguments  
(e.g. `bench eval graphwalks --model ollama/llama3.1:70b -T task=parents`)

### Generation Settings

| CLI Command        | Environment Variable     | Description                                   |
|--------------------|--------------------------|-----------------------------------------------|
| `--temperature`    | `BENCH_TEMPERATURE`      | Model sampling temperature.                   |
| `--top-p`          | `BENCH_TOP_P`            | Nucleus sampling.                             |
| `--max-tokens`     | `BENCH_MAX_TOKENS`       | Maximum tokens for model response.            |
| `--seed`           | `BENCH_SEED`             | Random seed for deterministic generation.     |

### Performance & Concurrency

| CLI Command        | Environment Variable     | Description                                   |
|--------------------|--------------------------|-----------------------------------------------|
| `--max-connections` | `BENCH_MAX_CONNECTIONS` | Maximum number of parallel requests.          |
| `--max-subprocesses`| `BENCH_MAX_SUBPROCESSES`| Maximum number of parallel subprocesses.      |
| `--timeout`        | `BENCH_TIMEOUT`          | Timeout per model API request (seconds).      |
| `--max-retries`    | `BENCH_MAX_RETRIES`      | Maximum number of retries for API requests.   |
| `--retry-on-error` | `BENCH_RETRY_ON_ERROR`   | Retry samples on errors (set number of retries). |
| `--fail-on-error`  | `BENCH_FAIL_ON_ERROR`    | Failure threshold for sample errors (percentage or count). |
| `--no-fail-on-error` | `BENCH_NO_FAIL_ON_ERROR` | Do not fail evaluation if errors occur.      |

### Logging & Output

| CLI Flag        | Environment Variable   | Description                                   |
|--------------------|------------------------|-----------------------------------------------|
| `--logfile`        | `BENCH_OUTPUT`         | Output file for results.                      |
| `--log-format`     | `BENCH_LOG_FORMAT`     | Output logging format (`eval` / `json`).      |
| `--display`        | `BENCH_DISPLAY`        | Display type for evaluation progress.         |
| `--log-samples / --no-log-samples` | `BENCH_LOG_SAMPLES` | Include or exclude detailed samples in logs. |
| `--log-images / --no-log-images`   | `BENCH_LOG_IMAGES`  | Include or exclude base64 encoded images in logs. |
| `--log-buffer`     | `BENCH_LOG_BUFFER`     | Number of samples to buffer before writing logs. |
| `--log-dir`        | `BENCH_LOG_DIR`        | Directory for log files.                      |

### Sandbox & Execution

| CLI Flag        | Environment Variable       | Description                                   |
|--------------------|----------------------------|-----------------------------------------------|
| `--sandbox`        | `BENCH_SANDBOX`            | Environment to run evaluation (`local` or `docker`). |
| `--sandbox-cleanup / --no-sandbox-cleanup` | `BENCH_SANDBOX_CLEANUP` | Cleanup sandbox environments after tasks (or skip cleanup). |

### Hub Integration

| CLI Flag        | Environment Variable   | Description                                   |
|--------------------|------------------------|-----------------------------------------------|
| `--hub-repo`       | `BENCH_HUB_REPO`       | Target Hub dataset repo for logs.             |
| `--hub-private`    | `BENCH_HUB_PRIVATE`    | Push Hub dataset as private.                  |

### Debugging & Inspection

| CLI Flag        | Environment Variable   | Description                                   |
|--------------------|------------------------|-----------------------------------------------|
| `--debug`          | `BENCH_DEBUG`          | Enable debug mode with full stack traces.     |
| `--debug-errors`   | `BENCH_DEBUG_ERRORS`   | Enable debug mode for errors only.            |
| `--trace`          | `BENCH_TRACE`          | Trace message interactions with model.        |
| `--alpha`          | `BENCH_ALPHA`          | Allow running experimental/alpha benchmarks.  |

## Configuration Examples

### Simple Custom Configuration

```bash
bench eval humaneval \
  --model openai/gpt-4o \
  --temperature 0.5 \
  --max-tokens 512 \
  --epochs 5 \
  --sandbox docker
```

### Model Comparison

```bash
bench eval mmlu --limit 500 \
    --model groq/llama-3.3-70b \
    --model openai/gpt-4o \
    --model anthropic/claude-3-5-sonnet
```

### Multi-Model Evaluation

```bash
bench eval simpleqa \
  --model-role candidate=groq/llama-3.3-70b \
  --model-role grader=openai/gpt-4o
```

### Batch Configuration

```bash
bench eval --model openai/gpt-4.1 \
  math --limit 20 \
  mmlu --limit 20
```

### Interruption Retry Configuration

```bash
bench eval-retry logs/incomplete.json \
  --max-retries 3 \
  --retry-on-error
```

### Display Configuration

```bash
# Display modes
export BENCH_DISPLAY="rich"           # Rich terminal output (default)
export BENCH_DISPLAY="plain"          # Simple text output
export BENCH_DISPLAY="none"           # No output (logs only)
export BENCH_DISPLAY="conversation"   # Show full conversations

bench eval mmlu --display conversation
```
